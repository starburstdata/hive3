***************
*** 1336,1344 ****
                dbName, tableName);
          }
          tTable = getMSC().getTable(getDefaultCatalog(conf), dbName, tableName,
-             validWriteIdList != null ? validWriteIdList.toString() : null, getColumnStats);
        } else {
-         tTable = getMSC().getTable(dbName, tableName, getColumnStats);
        }
      } catch (NoSuchObjectException e) {
        if (throwException) {
--- 1337,1345 ----
                dbName, tableName);
          }
          tTable = getMSC().getTable(getDefaultCatalog(conf), dbName, tableName,
+             validWriteIdList != null ? validWriteIdList.toString() : null, getColumnStats, Constants.HIVE_ENGINE);
        } else {
+         tTable = getMSC().getTable(dbName, tableName, getColumnStats, Constants.HIVE_ENGINE);
        }
      } catch (NoSuchObjectException e) {
        if (throwException) {
***************
*** 3788,3794 ****
        for (int i = 0; i < nBatches; ++i) {
          List<org.apache.hadoop.hive.metastore.api.Partition> tParts =
            getMSC().getPartitionsByNames(tbl.getDbName(), tbl.getTableName(),
-             partNames.subList(i*batchSize, (i+1)*batchSize), getColStats);
          if (tParts != null) {
            for (org.apache.hadoop.hive.metastore.api.Partition tpart: tParts) {
              partitions.add(new Partition(tbl, tpart));
--- 3789,3795 ----
        for (int i = 0; i < nBatches; ++i) {
          List<org.apache.hadoop.hive.metastore.api.Partition> tParts =
            getMSC().getPartitionsByNames(tbl.getDbName(), tbl.getTableName(),
+             partNames.subList(i*batchSize, (i+1)*batchSize), getColStats, Constants.HIVE_ENGINE);
          if (tParts != null) {
            for (org.apache.hadoop.hive.metastore.api.Partition tpart: tParts) {
              partitions.add(new Partition(tbl, tpart));
***************
*** 3799,3805 ****
        if (nParts > nBatches * batchSize) {
          List<org.apache.hadoop.hive.metastore.api.Partition> tParts =
            getMSC().getPartitionsByNames(tbl.getDbName(), tbl.getTableName(),
-             partNames.subList(nBatches*batchSize, nParts), getColStats);
          if (tParts != null) {
            for (org.apache.hadoop.hive.metastore.api.Partition tpart: tParts) {
              partitions.add(new Partition(tbl, tpart));
--- 3800,3806 ----
        if (nParts > nBatches * batchSize) {
          List<org.apache.hadoop.hive.metastore.api.Partition> tParts =
            getMSC().getPartitionsByNames(tbl.getDbName(), tbl.getTableName(),
+             partNames.subList(nBatches*batchSize, nParts), getColStats, Constants.HIVE_ENGINE);
          if (tParts != null) {
            for (org.apache.hadoop.hive.metastore.api.Partition tpart: tParts) {
              partitions.add(new Partition(tbl, tpart));
***************
*** 5181,5190 ****
        if (checkTransactional) {
          Table tbl = getTable(dbName, tableName);
          AcidUtils.TableSnapshot tableSnapshot = AcidUtils.getTableSnapshot(conf, tbl);
-         retv = getMSC().getTableColumnStatistics(dbName, tableName, colNames,
              tableSnapshot != null ? tableSnapshot.getValidWriteIdList() : null);
        } else {
-         retv = getMSC().getTableColumnStatistics(dbName, tableName, colNames);
        }
        return retv;
      } catch (Exception e) {
--- 5182,5191 ----
        if (checkTransactional) {
          Table tbl = getTable(dbName, tableName);
          AcidUtils.TableSnapshot tableSnapshot = AcidUtils.getTableSnapshot(conf, tbl);
+         retv = getMSC().getTableColumnStatistics(dbName, tableName, colNames, Constants.HIVE_ENGINE,
              tableSnapshot != null ? tableSnapshot.getValidWriteIdList() : null);
        } else {
+         retv = getMSC().getTableColumnStatistics(dbName, tableName, colNames, Constants.HIVE_ENGINE);
        }
        return retv;
      } catch (Exception e) {
***************
*** 5206,5212 ****
        }
  
        return getMSC().getPartitionColumnStatistics(
-           dbName, tableName, partNames, colNames, writeIdList);
      } catch (Exception e) {
        LOG.debug(StringUtils.stringifyException(e));
        throw new HiveException(e);
--- 5207,5213 ----
        }
  
        return getMSC().getPartitionColumnStatistics(
+           dbName, tableName, partNames, colNames, Constants.HIVE_ENGINE, writeIdList);
      } catch (Exception e) {
        LOG.debug(StringUtils.stringifyException(e));
        throw new HiveException(e);
***************
*** 5222,5228 ****
          AcidUtils.TableSnapshot tableSnapshot = AcidUtils.getTableSnapshot(conf, tbl);
          writeIdList = tableSnapshot != null ? tableSnapshot.getValidWriteIdList() : null;
        }
-       return getMSC().getAggrColStatsFor(dbName, tblName, colNames, partName, writeIdList);
      } catch (Exception e) {
        LOG.debug(StringUtils.stringifyException(e));
        return new AggrStats(new ArrayList<ColumnStatisticsObj>(),0);
--- 5223,5229 ----
          AcidUtils.TableSnapshot tableSnapshot = AcidUtils.getTableSnapshot(conf, tbl);
          writeIdList = tableSnapshot != null ? tableSnapshot.getValidWriteIdList() : null;
        }
+       return getMSC().getAggrColStatsFor(dbName, tblName, colNames, partName, Constants.HIVE_ENGINE, writeIdList);
      } catch (Exception e) {
        LOG.debug(StringUtils.stringifyException(e));
        return new AggrStats(new ArrayList<ColumnStatisticsObj>(),0);
